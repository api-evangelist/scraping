[
	{
		"id": "152",
		"name": "AlchemyAPI",
		"summary": "The product of over 50 person years of engineering effort AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field Used over 3 billion times every month AlchemyAPI enables customers to perform largescale social media monitoring target advertisements more effectively track influencers and sentiment within th",
		"details": "The product of over 50 person years of engineering effort AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field Used over 3 billion times every month AlchemyAPI enables customers to perform largescale social media monitoring target advertisements more effectively track influencers and sentiment within the media automate content aggregation and recommendation make more accurate stock trading decisions enhance business and government intelligence systems and create smarter applications and services",
		"website": "http://www.alchemyapi.com/",
		"twitter": "https://twitter.com/alchemyapi",
		"github": "https://github.com/AlchemyAPI",
		"blog": "http://blog.alchemyapi.com/",
		"blogrss": "http://www.alchemyapi.com/blog.xml?feed=rss2",
		"logo": "https://avatars.githubusercontent.com/u/4061718?v=2",
		"logo_width": "175",
		"screenshot": "https://avatars.githubusercontent.com/u/4061718?v=2",
		"tags": "Content Analysis,Machine-Learning-Stack,Scraping,Semantic Analysis,Text"
	},
	{
		"id": "1920",
		"name": "CommonCrawl",
		"summary": "Common Crawl is a nonprofit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone Common Crawl Foundation is a California 501c3 registered nonprofit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzabl",
		"details": "Common Crawl is a nonprofit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone Common Crawl Foundation is a California 501c3 registered nonprofit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzable",
		"website": "http://commoncrawl.org/",
		"twitter": "https://twitter.com/CommonCrawl",
		"github": "https://github.com/commoncrawl",
		"blog": "http://commoncrawl.org/blog/",
		"blogrss": "http://commoncrawl.org/feed/",
		"logo": "https://avatars.githubusercontent.com/u/1194841?v=2",
		"logo_width": "175",
		"screenshot": "https://avatars.githubusercontent.com/u/1194841?v=2",
		"tags": "Scraping,Search-Stack"
	},
	{
		"id": "1922",
		"name": "ConvExtra",
		"summary": "Convextra allows you collect valuable data from internet and represents it in easytouse CVS format for forther utilization",
		"details": "Convextra allows you collect valuable data from internet and represents it in easytouse CVS format for forther utilization",
		"website": "http://convextra.com/",
		"twitter": "",
		"github": "",
		"blog": "",
		"blogrss": "",
		"logo": null,
		"logo_width": "175",
		"screenshot": null,
		"tags": "Scraping,Scraping-Stack"
	},
	{
		"id": "1728",
		"name": "import.io",
		"summary": "Importio turns the web into a database releasing the vast potential of data trapped in websites Allowing you to identify a website select the data and treat it as a table in your database In effect transform the data into a row and column format You can then add more websites to your data set the same as adding more rows and query in realtime to access the data",
		"details": "Importio turns the web into a database releasing the vast potential of data trapped in websites Allowing you to identify a website select the data and treat it as a table in your database In effect transform the data into a row and column format You can then add more websites to your data set the same as adding more rows and query in realtime to access the data",
		"website": "http://docs.import.io/",
		"twitter": "https://twitter.com/importio",
		"github": "https://github.com/import-io",
		"blog": "http://blog.import.io/",
		"blogrss": "",
		"logo": "http://pbs.twimg.com/profile_images/473741216320729088/xuc8kDu__normal.png",
		"logo_width": "150",
		"screenshot": "http://pbs.twimg.com/profile_images/473741216320729088/xuc8kDu__normal.png",
		"tags": "Analytics,API Deployment,API Deployment,API-Scraping-Deployment,API-United-Kingdom,Data,Scraping,Scraping-Stack"
	},
	{
		"id": "2212",
		"name": "Kimono Labs",
		"summary": "Kimono is a way to turn websites into structured APIs from your browser in seconds You donrsquot need to write any code or install any software to extract data with Kimono The easiest way to use Kimono is to add our bookmarklet to your browserrsquos bookmark bar Then go to the website you want to get data from and click the bookmarklet Select the data you want and Kimono does the rest",
		"details": "Kimono is a way to turn websites into structured APIs from your browser in seconds You donrsquot need to write any code or install any software to extract data with Kimono The easiest way to use Kimono is to add our bookmarklet to your browserrsquos bookmark bar Then go to the website you want to get data from and click the bookmarklet Select the data you want and Kimono does the rest",
		"website": "https://www.kimonolabs.com/",
		"twitter": "https://twitter.com/kimonolabs",
		"github": "",
		"blog": "http://blog.kimonolabs.com/",
		"blogrss": "http://blog.kimonolabs.com/feed/",
		"logo": "http://pbs.twimg.com/profile_images/436682081913954305/4fPP_eD4_normal.png",
		"logo_width": "140",
		"screenshot": "http://pbs.twimg.com/profile_images/436682081913954305/4fPP_eD4_normal.png",
		"tags": "API Deployment,API-Scraping-Deployment,Scraping,Spreadsheets"
	},
	{
		"id": "1500",
		"name": "Motyar",
		"summary": "Scrape web without writing code for it To create value from the sea of data being published over web Data is CurrencyAPI Web scrape master provides a very simple API for retrieving scrape datanbsp",
		"details": "Scrape web without writing code for it To create value from the sea of data being published over web Data is CurrencyAPI Web scrape master provides a very simple API for retrieving scrape datanbsp",
		"website": "http://motyar.info/webscrapemaster/",
		"twitter": "https://twitter.com/motyar",
		"github": "",
		"blog": "http://motyar.blogspot.com/",
		"blogrss": "http://motyar.blogspot.com/feeds/posts/default?alt=rss",
		"logo": "http://pbs.twimg.com/profile_images/470000934546714624/SlZPCA21_normal.jpeg",
		"logo_width": "175",
		"screenshot": "http://pbs.twimg.com/profile_images/470000934546714624/SlZPCA21_normal.jpeg",
		"tags": "Content,Data,NLP,Scraping"
	},
	{
		"id": "1919",
		"name": "Mozenda",
		"summary": "Web data extraction and mashups are easy with Mozenda Were industry leaders in screen scraping and data integration",
		"details": "Web data extraction and mashups are easy with Mozenda Were industry leaders in screen scraping and data integration",
		"website": "http://www.mozenda.com",
		"twitter": "http://www.twitter.com/mozenda",
		"github": "",
		"blog": "http://www.mozenda.com/blog/",
		"blogrss": "http://www.mozenda.com/blog/?feed=rss2",
		"logo": "http://pbs.twimg.com/profile_images/742642971/MozendaIcon73-twitter_normal.png",
		"logo_width": "150",
		"screenshot": "http://pbs.twimg.com/profile_images/742642971/MozendaIcon73-twitter_normal.png",
		"tags": "Scraping"
	},
	{
		"id": "1569",
		"name": "PageMunch",
		"summary": "Page Munch is a simple API that allows you to turn web pages into rich structured JSON Easily extract photos videos event author and other metadata from any page on the internet in milliseconds",
		"details": "Page Munch is a simple API that allows you to turn web pages into rich structured JSON Easily extract photos videos event author and other metadata from any page on the internet in milliseconds",
		"website": "http://www.pagemunch.com",
		"twitter": "https://twitter.com/PageMunch",
		"github": "https://github.com/PageMunch",
		"blog": "",
		"blogrss": "",
		"logo": "https://avatars.githubusercontent.com/u/3577440?",
		"logo_width": "80",
		"screenshot": "https://avatars.githubusercontent.com/u/3577440?",
		"tags": "Scraper,Scraping,Utility"
	},
	{
		"id": "11394",
		"name": "ParseHub",
		"summary": "Turn dynamic websites into APIs You can extract data from anywhere ParseHub works with singlepage apps multipage apps and just about any other modern web technology ParseHub can handle Javascript AJAX cookies sessions and redirects You can easily fill in forms loop through dropdowns login to websites click on interactive maps and even deal with infinite scrolling",
		"details": "Turn dynamic websites into APIs You can extract data from anywhere ParseHub works with singlepage apps multipage apps and just about any other modern web technology ParseHub can handle Javascript AJAX cookies sessions and redirects You can easily fill in forms loop through dropdowns login to websites click on interactive maps and even deal with infinite scrolling",
		"website": "https://www.parsehub.com/",
		"twitter": "https://twitter.com/parsehub",
		"github": "",
		"blog": "",
		"blogrss": "",
		"logo": "http://pbs.twimg.com/profile_images/514503593299177472/CuWFB47I_normal.png",
		"logo_width": "150",
		"screenshot": "http://pbs.twimg.com/profile_images/514503593299177472/CuWFB47I_normal.png",
		"tags": "Scraping,Scraping-Stack"
	},
	{
		"id": "2511",
		"name": "PromptCloud",
		"summary": "PromptCloud opeartes on ldquoData as a Servicerdquo DaaS model and deals with largescale data crawl and extraction using cuttingedge technologies and cloud computing solutions Nutch Hadoop Lucene Cassandra etc Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format These data could be from reviews blogs product catalogs social s",
		"details": "PromptCloud opeartes on ldquoData as a Servicerdquo DaaS model and deals with largescale data crawl and extraction using cuttingedge technologies and cloud computing solutions Nutch Hadoop Lucene Cassandra etc Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format These data could be from reviews blogs product catalogs social sites travel datamdashbasically anything and everything on WWW Itrsquos a customized solution over simply being a massdata crawler so you only get the data you wish to see The solution provides both deep crawl and refresh crawl of the web pages in a structured format",
		"website": "http://promptcloud.com/",
		"twitter": "https://twitter.com/promptcloud",
		"github": "https://github.com/promptcloud",
		"blog": "http://blog.promptcloud.com/",
		"blogrss": "http://blog.promptcloud.com/feeds/posts/default?alt=rss",
		"logo": "http://pbs.twimg.com/profile_images/494731844516995072/iYT5qkv6_normal.png",
		"logo_width": "150",
		"screenshot": "http://pbs.twimg.com/profile_images/494731844516995072/iYT5qkv6_normal.png",
		"tags": "Scraping,Scraping-Stack"
	},
	{
		"id": "1825",
		"name": "ScrapeLogo",
		"summary": "ScrapeLogo has been discovered and developed by Maintop Businesses originally only for internal purposes It was coded as an independent service for several Maintoprsquos B2B projects When requests from other companies multiplied a private beta version was launched too We are now looking for the first beta testers who would like to show company logos on their websites and help us improve the qualit",
		"details": "ScrapeLogo has been discovered and developed by Maintop Businesses originally only for internal purposes It was coded as an independent service for several Maintoprsquos B2B projects When requests from other companies multiplied a private beta version was launched too We are now looking for the first beta testers who would like to show company logos on their websites and help us improve the quality and precision of our algorithm",
		"website": "http://scrapelogo.com/",
		"twitter": "",
		"github": "",
		"blog": "",
		"blogrss": "",
		"logo": null,
		"logo_width": "150",
		"screenshot": null,
		"tags": "Logo,Marketing,Scraping,Scraping-Stack"
	},
	{
		"id": "247",
		"name": "ScraperWiki",
		"summary": "ScraperWiki is a webbased platform for collaboratively building programs to extract and analyze public online data in a wikilike fashion Scraper refers to screen scrapers programs that extract data from websites Wiki means that any user with programming experience can create or edit such programs for extracting new data or for analyzing existing datasets The main use of the website is providing a ",
		"details": "ScraperWiki is a webbased platform for collaboratively building programs to extract and analyze public online data in a wikilike fashion Scraper refers to screen scrapers programs that extract data from websites Wiki means that any user with programming experience can create or edit such programs for extracting new data or for analyzing existing datasets The main use of the website is providing a place for programmers and journalists to collaborate on analyzing public data",
		"website": "https://scraperwiki.com/",
		"twitter": "https://twitter.com/scraperwiki",
		"github": "https://github.com/scraperwiki",
		"blog": "https://blog.scraperwiki.com/blog/",
		"blogrss": "https://blog.scraperwiki.com/feed/",
		"logo": "https://avatars.githubusercontent.com/u/1050343?",
		"logo_width": "175",
		"screenshot": "https://avatars.githubusercontent.com/u/1050343?",
		"tags": "API-Scraping-Deployment,API-United-Kingdom,Hacker Storytelling,Harvesting,Scraping,Spreadsheets"
	},
	{
		"id": "2509",
		"name": "Scrapinghub",
		"summary": "Scrapinghub is a company that provides web crawling solutions including a platform for running crawlers a tool for building scrapers visually data feed providers DaaS and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures",
		"details": "Scrapinghub is a company that provides web crawling solutions including a platform for running crawlers a tool for building scrapers visually data feed providers DaaS and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures",
		"website": "http://scrapinghub.com/",
		"twitter": "https://twitter.com/ScrapingHub",
		"github": "https://github.com/scrapinghub",
		"blog": "http://blog.scrapinghub.com/",
		"blogrss": "http://blog.scrapinghub.com/feed/",
		"logo": null,
		"logo_width": "150",
		"screenshot": null,
		"tags": "Scraping,Scraping-Stacking"
	},
	{
		"id": "11435",
		"name": "Scrapy",
		"summary": "An open source and collaborative framework for extracting the data you need from websites In a fast simple yet extensible way Scrapy is a fast highlevel screen scraping and web crawling framework used to crawl websites and extract structured data from their pages It can be used for a wide range of purposes from data mining to monitoring and automated testing",
		"details": "An open source and collaborative framework for extracting the data you need from websites In a fast simple yet extensible way Scrapy is a fast highlevel screen scraping and web crawling framework used to crawl websites and extract structured data from their pages It can be used for a wide range of purposes from data mining to monitoring and automated testing",
		"website": "http://scrapy.org/",
		"twitter": "https://twitter.com/ScrapyProject",
		"github": "https://github.com/scrapy",
		"blog": "",
		"blogrss": "",
		"logo": null,
		"logo_width": "150",
		"screenshot": null,
		"tags": "Harvesting,Scraping"
	},
	{
		"id": "1921",
		"name": "Screen Scraper",
		"summary": "Copying text from a web page Clicking links Entering data into forms and submitting Iterating through search results pages Downloading files PDF MS Word images etc",
		"details": "Copying text from a web page Clicking links Entering data into forms and submitting Iterating through search results pages Downloading files PDF MS Word images etc",
		"website": "http://screen-scraper.com/",
		"twitter": "",
		"github": "",
		"blog": "http://blog.screen-scraper.com/",
		"blogrss": "http://feeds.feedburner.com/screen-scrapeable",
		"logo": null,
		"logo_width": "175",
		"screenshot": null,
		"tags": "Scraping,Scraping-Stack"
	}
]