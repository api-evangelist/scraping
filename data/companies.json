{"company":[{"id":"152","name":"AlchemyAPI","summary":"The product of over 50 person years of engineering effort, AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field. Used over 3 billion times every month, AlchemyAPI enables customers to perform large-scale social media monitoring, target advertisements more effectively, track influencers and sentiment wit","details":"The product of over 50 person years of engineering effort, AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field. Used over 3 billion times every month, AlchemyAPI enables customers to perform large-scale social media monitoring, target advertisements more effectively, track influencers and sentiment within the media, automate content aggregation and recommendation, make more accurate stock trading decisions, enhance business and government intelligence systems, and create smarter applications and services.","website":"http:\/\/www.alchemyapi.com\/","twitter":"https:\/\/twitter.com\/alchemyapi","github":"https:\/\/github.com\/AlchemyAPI","blog":"http:\/\/blog.alchemyapi.com\/","blogrss":"http:\/\/www.alchemyapi.com\/blog.xml?feed=rss2","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/alchemy-api-logo.png","logo_width":"175","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/ap-evangelist-site\/api\/screenshots\/171_alchemyapi_text_categorization.png","tags":"Content Analysis,Scraping,Semantic Analysis,Stack,Text"},{"id":"1920","name":"CommonCrawl","summary":"Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and an","details":"Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzable.","website":"http:\/\/commoncrawl.org\/","twitter":"https:\/\/twitter.com\/CommonCrawl","github":"https:\/\/github.com\/commoncrawl","blog":"http:\/\/commoncrawl.org\/blog\/","blogrss":"http:\/\/commoncrawl.org\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/common-crawl-logo-2.png","logo_width":"175","screenshot":null,"tags":"Scraping"},{"id":"1922","name":"ConvExtra","summary":"Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.","details":"Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.","website":"http:\/\/convextra.com\/","twitter":"","github":"","blog":"","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/convextra-logo.png","logo_width":"175","screenshot":null,"tags":"Scraping"},{"id":"1728","name":"import.io","summary":"Importio turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.","details":"Importio turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.","website":"http:\/\/docs.import.io\/","twitter":"https:\/\/twitter.com\/importio","github":"https:\/\/github.com\/import-io","blog":"http:\/\/blog.import.io\/","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/1728_logo.png","logo_width":"150","screenshot":null,"tags":"Analytics,API-Deployment,API-Scraping-Deployment,API-United-Kingdom,Data,Scraping"},{"id":"1919","name":"Mozenda","summary":false,"details":"","website":"http:\/\/www.mozenda.com","twitter":"http:\/\/www.twitter.com\/mozenda","github":"","blog":"http:\/\/www.mozenda.com\/blog\/","blogrss":"http:\/\/www.mozenda.com\/blog\/?feed=rss2","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/mozenda-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping"},{"id":"1569","name":"PageMunch","summary":"Page Munch is a simple API that allows you to turn web pages into rich, structured JSON. Easily extract photos, videos, event, author and other metadata from any page on the internet in milliseconds.","details":"Page Munch is a simple API that allows you to turn web pages into rich, structured JSON. Easily extract photos, videos, event, author and other metadata from any page on the internet in milliseconds.","website":"http:\/\/www.pagemunch.com","twitter":"https:\/\/twitter.com\/PageMunch","github":"https:\/\/github.com\/PageMunch","blog":"","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/page-munch-logo.png","logo_width":"80","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/ap-evangelist-site\/api\/screenshots\/8900_pagemunch.png","tags":"scraper,Scraping,Scraping-Stack,Stack,Utility"},{"id":"1825","name":"ScrapeLogo","summary":"ScrapeLogo has been discovered and developed by Maintop Businesses, originally only for internal purposes. It was coded as an independent service for several Maintop&rsquo;s B2B projects. When requests from other companies multiplied, a private beta version was launched too. We are now looking for the first beta testers, who would like to show company logos on their websites and help us improve th","details":"ScrapeLogo has been discovered and developed by Maintop Businesses, originally only for internal purposes. It was coded as an independent service for several Maintop&rsquo;s B2B projects. When requests from other companies multiplied, a private beta version was launched too. We are now looking for the first beta testers, who would like to show company logos on their websites and help us improve the quality and precision of our algorithm.","website":"http:\/\/scrapelogo.com\/","twitter":"","github":"","blog":"","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/scrape-logo.png","logo_width":"150","screenshot":null,"tags":"Logo,Marketing,Scraping,Scraping-Stack"},{"id":"247","name":"ScraperWiki","summary":"ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. \"Scraper\" refers to screen scrapers, programs that extract data from websites. \"Wiki\" means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website i","details":"ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. \"Scraper\" refers to screen scrapers, programs that extract data from websites. \"Wiki\" means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website is providing a place for programmers and journalists to collaborate on analyzing public data","website":"https:\/\/scraperwiki.com\/","twitter":"https:\/\/twitter.com\/scraperwiki","github":"https:\/\/github.com\/scraperwiki","blog":"https:\/\/blog.scraperwiki.com\/blog\/","blogrss":"https:\/\/blog.scraperwiki.com\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/798px-ScraperWiki_logo.png","logo_width":"175","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/api\/ScraperWiki-Screenshot.png","tags":"API-Scraping-Deployment,API-United-Kingdom,Data,Hacker Storytelling,Harvesting,Scraping,Scraping-Stack,Spreadsheets,Stack"},{"id":"1921","name":"Screen Scraper","summary":"Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).","details":"Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).","website":"http:\/\/screen-scraper.com\/","twitter":"","github":"","blog":"http:\/\/blog.screen-scraper.com\/","blogrss":"http:\/\/feeds.feedburner.com\/screen-scrapeable","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/screen-scraper-logo.png","logo_width":"175","screenshot":null,"tags":"Scraping,Scraping-Stack"},{"id":"1500","name":"Motyar","summary":"Scrape web without writing code for it; To create value from the sea of data being published over web. Data is Currency.API. Web scrape master provides a very simple API for retrieving scrape data.","details":"Scrape web without writing code for it; To create value from the sea of data being published over web. Data is Currency.API. Web scrape master provides a very simple API for retrieving scrape data.","website":"http:\/\/motyar.info\/webscrapemaster\/","twitter":"https:\/\/twitter.com\/motyar","github":"","blog":"http:\/\/motyar.blogspot.com\/","blogrss":"http:\/\/motyar.blogspot.com\/feeds\/posts\/default?alt=rss","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/web-scrape-master-logo-2.png","logo_width":"175","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/ap-evangelist-site\/api\/screenshots\/8364_web_scrape_master.png","tags":"Content,Data,NLP,Scraping,Stack,Tools"},{"id":"2212","name":"Kimono Labs","summary":"Kimono is a way to turn websites into structured APIs from your browser in seconds. You don&rsquo;t need to write any code or install any software to extract data with Kimono. The easiest way to use Kimono is to add our bookmarklet to your browser&rsquo;s bookmark bar. Then go to the website you want to get data from and click the bookmarklet. Select the data you want and Kimono does the rest.","details":"Kimono is a way to turn websites into structured APIs from your browser in seconds. You don&rsquo;t need to write any code or install any software to extract data with Kimono. The easiest way to use Kimono is to add our bookmarklet to your browser&rsquo;s bookmark bar. Then go to the website you want to get data from and click the bookmarklet. Select the data you want and Kimono does the rest.","website":"https:\/\/www.kimonolabs.com\/","twitter":"https:\/\/twitter.com\/kimonolabs","github":"","blog":"http:\/\/blog.kimonolabs.com\/","blogrss":"http:\/\/blog.kimonolabs.com\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/kimono-labs-logo.png","logo_width":"140","screenshot":null,"tags":"API-Deployment,API-Scraping-Deployment,Scraping,Spreadsheets"},{"id":"2511","name":"PromptCloud","summary":"PromptCloud opeartes on &ldquo;Data as a Service&rdquo; (DaaS) model and deals with large-scale data crawl and extraction, using cutting-edge technologies and cloud computing solutions (Nutch, Hadoop, Lucene, Cassandra, etc). Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format. These data could be from reviews, blogs, produc","details":"PromptCloud opeartes on &ldquo;Data as a Service&rdquo; (DaaS) model and deals with large-scale data crawl and extraction, using cutting-edge technologies and cloud computing solutions (Nutch, Hadoop, Lucene, Cassandra, etc). Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format. These data could be from reviews, blogs, product catalogs, social sites, travel data&mdash;basically anything and everything on WWW. It&rsquo;s a customized solution over simply being a mass-data crawler, so you only get the data you wish to see. The solution provides both deep crawl and refresh crawl of the web pages in a structured format.","website":"http:\/\/promptcloud.com\/","twitter":"https:\/\/twitter.com\/promptcloud","github":"https:\/\/github.com\/promptcloud","blog":"http:\/\/blog.promptcloud.com\/","blogrss":"http:\/\/blog.promptcloud.com\/feeds\/posts\/default?alt=rss","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/prompt-cloud-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping,Scraping-Stack"},{"id":"2509","name":"Scrapinghub","summary":"Scrapinghub is a company that provides web crawling solutions, including a platform for running crawlers, a tool for building scrapers visually, data feed providers (DaaS) and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures.","details":"Scrapinghub is a company that provides web crawling solutions, including a platform for running crawlers, a tool for building scrapers visually, data feed providers (DaaS) and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures.","website":"http:\/\/scrapinghub.com\/","twitter":"https:\/\/twitter.com\/ScrapingHub","github":"https:\/\/github.com\/scrapinghub","blog":"http:\/\/blog.scrapinghub.com\/","blogrss":"http:\/\/blog.scrapinghub.com\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/scrapinghub-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping,Scraping-Stacking"}],"published":"08\/28\/2014"}