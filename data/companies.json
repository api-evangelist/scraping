{"company":[{"id":"1920","name":"Common Crawl","summary":"Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and an","details":"Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzable.","website":"http:\/\/commoncrawl.org\/","twitter":"https:\/\/twitter.com\/CommonCrawl","github":"https:\/\/github.com\/commoncrawl","blog":"http:\/\/commoncrawl.org\/blog\/","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/common-crawl-logo-2.png","logo_width":"150","screenshot":null,"tags":"Scraping"},{"id":"1922","name":"ConvExtra","summary":"Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.","details":"Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.","website":"http:\/\/convextra.com\/","twitter":"","github":"","blog":"","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/convextra-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping"},{"id":"1728","name":"import.io","summary":"Importio turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.","details":"Importio turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.","website":"http:\/\/docs.import.io\/","twitter":"https:\/\/twitter.com\/importio","github":"https:\/\/github.com\/import-io","blog":"http:\/\/blog.import.io\/","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/1728_logo.png","logo_width":"150","screenshot":null,"tags":"Analytics,API-United-Kingdom,Data,Scraping"},{"id":"1569","name":"PageMunch","summary":"Page Munch is a simple API that allows you to turn web pages into rich, structured JSON. Easily extract photos, videos, event, author and other metadata from any page on the internet in milliseconds.","details":"Page Munch is a simple API that allows you to turn web pages into rich, structured JSON. Easily extract photos, videos, event, author and other metadata from any page on the internet in milliseconds.","website":"http:\/\/www.pagemunch.com","twitter":"https:\/\/twitter.com\/PageMunch","github":"https:\/\/github.com\/PageMunch","blog":"","blogrss":"","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/page-munch-logo.png","logo_width":"125","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/ap-evangelist-site\/api\/screenshots\/8900_pagemunch.png","tags":"JSON,scraper,Scraping,Stack,Utility"},{"id":"247","name":"ScraperWiki","summary":"ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. \"Scraper\" refers to screen scrapers, programs that extract data from websites. \"Wiki\" means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website i","details":"ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. \"Scraper\" refers to screen scrapers, programs that extract data from websites. \"Wiki\" means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website is providing a place for programmers and journalists to collaborate on analyzing public data","website":"https:\/\/scraperwiki.com\/docs\/api","twitter":"https:\/\/twitter.com\/#!\/scraperwiki","github":"https:\/\/github.com\/scraperwiki","blog":"http:\/\/blog.scraperwiki.com\/","blogrss":"http:\/\/blog.scraperwiki.com\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/798px-ScraperWiki_logo.png","logo_width":"170","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/api\/ScraperWiki-Screenshot.png","tags":"API-United-Kingdom,Data,Hacker Storytelling,Harvesting,Scraping,Stack,Wiki"},{"id":"1921","name":"Screen Scraper","summary":"Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).","details":"Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).","website":"http:\/\/screen-scraper.com\/","twitter":"","github":"","blog":"http:\/\/blog.screen-scraper.com\/","blogrss":"http:\/\/feeds.feedburner.com\/screen-scrapeable","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/screen-scraper-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping"},{"id":"1500","name":"Web Scrape Master","summary":"Scrape web without writing code for it; To create value from the sea of data being published over web. Data is Currency.API. Web scrape master provides a very simple API for retrieving scrape data.","details":"Scrape web without writing code for it; To create value from the sea of data being published over web. Data is Currency.API. Web scrape master provides a very simple API for retrieving scrape data.","website":"http:\/\/motyar.info\/webscrapemaster\/","twitter":"https:\/\/twitter.com\/motyar","github":"","blog":"http:\/\/motyar.blogspot.com\/","blogrss":"http:\/\/motyar.blogspot.com\/feeds\/posts\/default?alt=rss","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/web-scrape-master-logo-2.png","logo_width":"150","screenshot":"http:\/\/kinlane-productions.s3.amazonaws.com\/ap-evangelist-site\/api\/screenshots\/8364_web_scrape_master.png","tags":"Content,Data,internet,NLP,REST,Scraping,Stack,Tools"},{"id":"2212","name":"Kimono","summary":"Kimono is a way to turn websites into structured APIs from your browser in seconds. You don&rsquo;t need to write any code or install any software to extract data with Kimono. The easiest way to use Kimono is to add our bookmarklet to your browser&rsquo;s bookmark bar. Then go to the website you want to get data from and click the bookmarklet. Select the data you want and Kimono does the rest.","details":"Kimono is a way to turn websites into structured APIs from your browser in seconds. You don&rsquo;t need to write any code or install any software to extract data with Kimono. The easiest way to use Kimono is to add our bookmarklet to your browser&rsquo;s bookmark bar. Then go to the website you want to get data from and click the bookmarklet. Select the data you want and Kimono does the rest.","website":"https:\/\/www.kimonolabs.com\/","twitter":"https:\/\/twitter.com\/kimonolabs","github":"","blog":"http:\/\/blog.kimonolabs.com\/","blogrss":"http:\/\/blog.kimonolabs.com\/feed\/","logo":"http:\/\/kinlane-productions.s3.amazonaws.com\/api-evangelist-site\/company\/logos\/kimono-labs-logo.png","logo_width":"150","screenshot":null,"tags":"Scraping"}],"published":"03\/31\/2014"}