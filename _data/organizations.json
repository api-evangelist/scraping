[
	{
		"id": "kCOmMg9AsRxnuSmxQm116xcUOHtV123fEQuhFJvM6TQrgB116cDAZ3R13600d5evKIb8LGRHydIRJEZWYfiBXs6K6DQw117117",
		"name": "AlchemyAPI",
		"summary": "The product of over 50 person years of engineering effort, AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field. Used over 3 billion times every month, AlchemyAPI enables customers to perform large-scale social media monitoring, target advertisements more effectively, track influencers and sentiment wit",
		"details": "The product of over 50 person years of engineering effort, AlchemyAPI is a text mining platform providing the most comprehensive set of semantic analysis capabilities in the natural language processing field. Used over 3 billion times every month, AlchemyAPI enables customers to perform large-scale social media monitoring, target advertisements more effectively, track influencers and sentiment within the media, automate content aggregation and recommendation, make more accurate stock trading decisions, enhance business and government intelligence systems, and create smarter applications and services.",
		"website": "http://www.alchemyapi.com/",
		"twitter": "https://twitter.com/alchemyapi",
		"github": "https://github.com/AlchemyAPI",
		"apisjson_url": "http://theapistack.com/data/alchemyapi/apis.json",
		"sdksio_url": "https://sdks.io/SDK/View/alchemyapi-3",
		"postman_url": "https://raw.githubusercontent.com/api-stack/api-stack/gh-pages/data/alchemyapi/alchemy-api-postman-collection.json",
		"portal_url": "https://github.com/AlchemyAPI",
		"base_url": "http://www.alchemyapi.com/developers",
		"blog": "http://blog.alchemyapi.com/",
		"blogrss": "http://blog.alchemyapi.com/rss.xml",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/alchemy-api-logo.png",
		"logo_width": "175",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/alchemy-api-logo.png",
		"tags": [
			"Semantic Analysis",
			"Scraping",
			"My API Stack",
			"Machine Learning Scraping",
			"Language",
			"Content Analysis",
			"Content"
		]
	},
	{
		"id": "PM3r7KVO5v6aUFJbvS1BR0Lf3p40HvyY1o7W4ybnGnzqc02XvuBMO36KqnuknGSNQFFg5h116n0MvKnudTBQWhvQ117117",
		"name": "Apache Nutch",
		"summary": "Nutch is a well matured, production ready Web crawler. Nutch 1.x enables fine grained configuration, relying on Apache Hadoopu0026trade; data structures, which are great for batch processing. - See more at: http://nutch.apache.org/index.html#sthash.dehuG4St.dpuf",
		"details": "Nutch is a well matured, production ready Web crawler. Nutch 1.x enables fine grained configuration, relying on Apache Hadoopu0026trade; data structures, which are great for batch processing. - See more at: http://nutch.apache.org/index.html#sthash.dehuG4St.dpuf",
		"website": "",
		"twitter": "",
		"github": "",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": "",
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/apache-nutch.jpg",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/apache-nutch.jpg",
		"tags": [
			"Scraping"
		]
	},
	{
		"id": "z4FNFbLf9k99bqPCYPW116XP3vpLMK116IP9j5ykdu90bMtIriVGR5UN3WjtS9gpGFV3eqILIMAVC1R0D7LIbzXHwQ117117",
		"name": "Apifier",
		"summary": "In late 2014, we needed a web scraper for one of our consulting projects, but couldnt find anything suitable. Therefore we decided to build a better scraper and it turned out people really liked it. Few months later, the project was selected with 32 others from 6500 applications to the inaugural Y Combinator Fellowship programme in August 2015. Apifier launched publicly in October 2015.",
		"details": "In late 2014, we needed a web scraper for one of our consulting projects, but couldnt find anything suitable. Therefore we decided to build a better scraper and it turned out people really liked it. Few months later, the project was selected with 32 others from 6500 applications to the inaugural Y Combinator Fellowship programme in August 2015. Apifier launched publicly in October 2015.",
		"website": "https://www.apifier.com/",
		"twitter": "https://twitter.com/ApifierInfo",
		"github": "https://github.com/apifier",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/apifier",
		"base_url": "https://www.apifier.com/docs",
		"blog": "https://blog.apifier.com/",
		"blogrss": "https://blog.apifier.com/feed",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Apifier_logo_h120.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Apifier_logo_h120.png",
		"tags": [
			"Web Crawler",
			"Scraping Web Crawler",
			"Scraping"
		]
	},
	{
		"id": "JQvndkfG6rF6wUim5gURrdkxm7GSl5YXHOSEE9TOiCoPNd2ytsIH1KHm123XdsrP43CpDumKuw3123jGs123HopNNRGw117117",
		"name": "Aylien",
		"summary": "AYLIEN Text Analysis API is a package of Natural Language Processing, Information Retrieval and Machine Learning tools for extracting meaning and insight from textual and visual content with ease. At AYLIEN, weu0026rsquo;re harnessing the potential of your data. Whether youre a news organization, a developer, a savvy marketer or an academic, youll soon see what a dose of AYLIEN intelligence can do for",
		"details": "AYLIEN Text Analysis API is a package of Natural Language Processing, Information Retrieval and Machine Learning tools for extracting meaning and insight from textual and visual content with ease. At AYLIEN, weu0026rsquo;re harnessing the potential of your data. Whether youre a news organization, a developer, a savvy marketer or an academic, youll soon see what a dose of AYLIEN intelligence can do for you. Our text API allows you to monitor the sentiment of your brand, analyze documents or summarize and classify large amounts of text.",
		"website": "http://aylien.com/",
		"twitter": "https://twitter.com/_aylien",
		"github": "https://github.com/AYLIEN",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/AYLIEN",
		"base_url": "https://newsapi.aylien.com/",
		"blog": "http://blog.aylien.com/",
		"blogrss": "http://blog.aylien.com/rss",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Full-transparent-background-512.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Full-transparent-background-512.png",
		"tags": [
			"Scraping",
			"Machine Learning Scraping",
			"Machine Learning",
			"Content"
		]
	},
	{
		"id": "dtLzAVvRBK2xxjazH5FZdpuifYDGk5T116UH1160MBJfhUyNYNS5he1pJjlAjVUqUO1QXkGxd9h123Z6jnr9BVancFnA117117",
		"name": "Bitext",
		"summary": "Bitext delivers the most precise and granular text analytics solution on the market, with an accuracy rate above 90%. We are computational linguists first. Our technology really understands sentence structure and its different layers of meaning, so it always produces the richest results.",
		"details": "Bitext delivers the most precise and granular text analytics solution on the market, with an accuracy rate above 90%. We are computational linguists first. Our technology really understands sentence structure and its different layers of meaning, so it always produces the richest results.",
		"website": "https://www.bitext.com/",
		"twitter": "https://twitter.com/Bitext",
		"github": "https://github.com/bitext",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/bitext",
		"base_url": "",
		"blog": "https://www.bitext.com/blog/",
		"blogrss": "https://www.bitext.com/feed/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/bitext-logo-620px.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/bitext-logo-620px.png",
		"tags": [
			"Scraping",
			"Machine Learning Scraping",
			"Machine Learning",
			"Content",
			"APIs.io Import"
		]
	},
	{
		"id": "2w9fLBSE1U7km6HSSGjBhnvzud116fJ6B8NihiGfLQyu2o2rVobeNZMiborJtsNh1wY12399SBP6WVrgx1Z7jWmgJA117117",
		"name": "CommonCrawl",
		"summary": "Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and an",
		"details": "Common Crawl is a non-profit foundation dedicated to providing an open repository of web crawl data that can be accessed and analyzed by everyone. Common Crawl Foundation is a California 501(c)(3) registered non-profit founded by Gil Elbaz with the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzable.",
		"website": "http://commoncrawl.org/",
		"twitter": "https://twitter.com/CommonCrawl",
		"github": "https://github.com/commoncrawl",
		"apisjson_url": "http://theapistack.com/data/commoncrawl/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/commoncrawl",
		"base_url": "",
		"blog": "http://commoncrawl.org/blog/",
		"blogrss": "http://commoncrawl.org/feed/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/common-crawl-logo-2.png",
		"logo_width": "175",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/common-crawl-logo-2.png",
		"tags": [
			"Search-Stack",
			"Scraping Web Crawler",
			"Scraping",
			"API-Stack"
		]
	},
	{
		"id": "WD2123xoBlxicqGSQxI4pkqYdu2FWlDw56HzhPTy3KT0TfQLqt9zQJOTHp116EuQ7OSF116M08hO9orAjKxD4RNzHfcQ117117",
		"name": "ConvExtra",
		"summary": "Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.",
		"details": "Convextra allows you collect valuable data from internet and represents it in easy-to-use CVS format for forther utilization.",
		"website": "http://convextra.com/",
		"twitter": "",
		"github": "",
		"apisjson_url": "http://theapistack.com/data/convextra/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": null,
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/convextra-logo.png",
		"logo_width": "175",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/convextra-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Scraping",
			"API-Stack"
		]
	},
	{
		"id": "wx0S106xuQ2cHbevzz8U1237bFPFulocR35JqRS8X8FCNUQs7qrnY0oZG63bYL123mCEMxGlRXPyy6GRem1qJnR5gg117117",
		"name": "Dandelion API",
		"summary": "Context Intelligence: from text to actionable data. Extract meaning from unstructured text and put it in context with a simple API.Thanks to its revolutionary technology, Dandelion API works well even on short and malformed texts in English, French, German, Italian and Portuguese.",
		"details": "Context Intelligence: from text to actionable data. Extract meaning from unstructured text and put it in context with a simple API.Thanks to its revolutionary technology, Dandelion API works well even on short and malformed texts in English, French, German, Italian and Portuguese.",
		"website": "https://dandelion.eu",
		"twitter": "https://twitter.com/dandelionapi",
		"github": "",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": "",
		"blog": "http://blog.dandelion.eu/",
		"blogrss": "http://blog.dandelion.eu/feed/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/dandelion-api.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/dandelion-api.png",
		"tags": [
			"Scraping",
			"Partners",
			"Machine-Learning-Stack",
			"Machine Learning Scraping",
			"Machine Learning"
		]
	},
	{
		"id": "hBSGEqF9J1YEYaW8fMXm9dS3r5fpE2aaqt116bNJOjGQKhvslDaRbWucPQrOkItSRghs8NWtg2JNpXJC4sgU3a6A117117",
		"name": "Diffbot",
		"summary": "u0026lt;pu0026gt;Diffbot provides a set of APIs that enable developers to easily use web data in their own applications. Diffbot analyzes documents much like a human would, using the visual properties to determine how the parts of the page fit together. The algorithm uses statistical techniques to automatically and reliably determine the structural organization of a page, independent of layout and the lan",
		"details": "u0026lt;pu0026gt;Diffbot provides a set of APIs that enable developers to easily use web data in their own applications. Diffbot analyzes documents much like a human would, using the visual properties to determine how the parts of the page fit together. The algorithm uses statistical techniques to automatically and reliably determine the structural organization of a page, independent of layout and the language of the text.",
		"website": "http://www.diffbot.com/our-apis/follow/",
		"twitter": "https://twitter.com/diffbot",
		"github": "https://github.com/diffbot",
		"apisjson_url": "http://theapistack.com/data/deskero/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/diffbot",
		"base_url": "",
		"blog": "http://diffbot.com/blog",
		"blogrss": "http://www.diffbot.com/api/rss/www.diffbot.com/blog",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/diffbot-logo.jpg",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/diffbot-logo.jpg",
		"tags": [
			"Scraping",
			"Machine Learning Scraping",
			"Machine Learning",
			"Extraction",
			"Deployment",
			"Crawler",
			"Content"
		]
	},
	{
		"id": "v8JAjFi53tKqc123vD116Bu3W1CoTRWT9s116kDDHtY8b4t15TwrBf1gDjWZOzPf3akxAuUpST7lSGBaqTEB123tVPMpvA117117",
		"name": "Embedly",
		"summary": "Extract allows you to mine important features within articlesu2014so you can use written content how you want to. Control colors, text, keywords, and entities in any article on your site. Remove extraneous information. As you automate the way you use articles, youu2019ll gain insight into your usersu2019 preferences, helping you serve them better.",
		"details": "Extract allows you to mine important features within articlesu2014so you can use written content how you want to. Control colors, text, keywords, and entities in any article on your site. Remove extraneous information. As you automate the way you use articles, youu2019ll gain insight into your usersu2019 preferences, helping you serve them better.",
		"website": "http://embed.ly/extract",
		"twitter": "https://twitter.com/embedly",
		"github": "https://github.com/embedly",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/embedly",
		"base_url": "",
		"blog": "https://twitter.com/weogeostatus",
		"blogrss": "http://blog.embed.ly/rss",
		"logo": "http://pbs.twimg.com/profile_images/3503494362/1d289aab136f2d111387a399f3a91b6a_normal.png",
		"logo_width": "100",
		"screenshot": "http://pbs.twimg.com/profile_images/3503494362/1d289aab136f2d111387a399f3a91b6a_normal.png",
		"tags": [
			"Video",
			"Social",
			"Scraping Tools",
			"Scraping",
			"Embeddable",
			"Embed"
		]
	},
	{
		"id": "ny5QIpkGNknxe0PxdI6m2GhOqfTq19HErArKPU0XGWg5BjMpM4arEiuc1163hL1Li123FgJB8SzEaDtXqvLLQ1165XVw117117",
		"name": "HPE Haven OnDemand",
		"summary": "HPE Haven OnDemand is a platform for building cognitive computing solutions using text analysis, speech recognition, image analysis, indexing and search APIs. Simply put, developers and businesses use APIs to add advanced capabilities such as natural language processing, machine learning, and predictive analytics to their applications.",
		"details": "HPE Haven OnDemand is a platform for building cognitive computing solutions using text analysis, speech recognition, image analysis, indexing and search APIs. Simply put, developers and businesses use APIs to add advanced capabilities such as natural language processing, machine learning, and predictive analytics to their applications.",
		"website": "https://www.havenondemand.com/",
		"twitter": "https://twitter.com/HavenOnDemand",
		"github": "https://github.com/HPE-Haven-OnDemand",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/HPE-Haven-OnDemand",
		"base_url": "https://dev.havenondemand.com/",
		"blog": "https://community.havenondemand.com/t5/Blog/bg-p/blog_iod",
		"blogrss": "https://community.havenondemand.com/etmmh37422/rss/boardmessages?board.id=blog_iod",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/hpe-haven-on-demand.jpeg",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/hpe-haven-on-demand.jpeg",
		"tags": [
			"Scraping",
			"Machine Learning Scraping",
			"Machine Learning"
		]
	},
	{
		"id": "Ja8ZQ3F0bXJxTKzxlxhhTcK70NZcl11603wB8OA1dP0L4GL1S7VCCwmCRSLJFGJ0XpDc9saa61233116MEwB8S1239Fddg117117",
		"name": "import.io",
		"summary": "Import.io turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.",
		"details": "Import.io turns the web into a database, releasing the vast potential of data trapped in websites. Allowing you to identify a website, select the data and treat it as a table in your database. In effect transform the data into a row and column format. You can then add more websites to your data set, the same as adding more rows and query in real-time to access the data.",
		"website": "http://docs.import.io/",
		"twitter": "https://twitter.com/importio",
		"github": "https://github.com/import-io",
		"apisjson_url": "http://theapistack.com/data/importio/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/import-io",
		"base_url": "",
		"blog": "http://blog.import.io/",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/1728_logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/1728_logo.png",
		"tags": [
			"Scraping",
			"Deployment",
			"Data",
			"API-United-Kingdom",
			"Analytics"
		]
	},
	{
		"id": "AuV11674guokWDRAU5oZgKrl8pMfHM9mvFBlJfGPF3PHjUrQF5aNNz1ArYbNlcgcIx2Ptk9uVETAFCRi0vRdYDxQ117117",
		"name": "Moz",
		"summary": "Moz is a software as a service (SaaS) company based in Seattle, Washington, U.S.A., that sells inbound marketing and marketing analytics software subscriptions. It was founded by Rand Fishkin and Gillian Muessig in 2004 as a consulting firm and shifted to software development in 2008. The company hosts a website which includes an online community of more than one million globally based digital mar",
		"details": "Moz is a software as a service (SaaS) company based in Seattle, Washington, U.S.A., that sells inbound marketing and marketing analytics software subscriptions. It was founded by Rand Fishkin and Gillian Muessig in 2004 as a consulting firm and shifted to software development in 2008. The company hosts a website which includes an online community of more than one million globally based digital marketers and marketing related tools.",
		"website": "https://moz.com",
		"twitter": "https://twitter.com/moz",
		"github": "https://github.com/seomoz",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/seomoz",
		"base_url": "https://moz.com/products/api",
		"blog": "https://moz.com/blog",
		"blogrss": "http://feedpress.me/mozblog",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/moz-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/moz-logo.png",
		"tags": [
			"Web Crawler",
			"Target",
			"Scraping Web Crawler",
			"Scraping",
			"Partners",
			"Marketing",
			"Links"
		]
	},
	{
		"id": "T116eEQuLCdJymEj6Xr123SRSQWsLShZVmXhSZ5e9ThOxfOUQvCCSdg67v3vLBJE88eIwQrpT1167E2vcFA3bd66DI7w117117",
		"name": "Mozenda",
		"summary": "Web data extraction and mashups are easy with Mozenda. Were industry leaders in screen scraping and data integration.",
		"details": "Web data extraction and mashups are easy with Mozenda. Were industry leaders in screen scraping and data integration.",
		"website": "http://www.mozenda.com",
		"twitter": "http://www.twitter.com/mozenda",
		"github": "",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": "",
		"blog": "http://www.mozenda.com/blog/",
		"blogrss": "http://www.mozenda.com/blog/?feed=rss2",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/mozenda-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/mozenda-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping"
		]
	},
	{
		"id": "Q6y9wvc123B0u1fjcaJSJdtcWCage9FBC72OnSdR3gsLfLgC1dpa6rd7cdYSOE2FVSCe2aybr811648As0kYv5ISig117117",
		"name": "ParseHub",
		"summary": "Turn dynamic websites into APIs. You can extract data from anywhere. ParseHub works with single-page apps, multi-page apps and just about any other modern web technology. ParseHub can handle Javascript, AJAX, cookies, sessions and redirects. You can easily fill in forms, loop through dropdowns, login to websites, click on interactive maps and even deal with infinite scrolling.",
		"details": "Turn dynamic websites into APIs. You can extract data from anywhere. ParseHub works with single-page apps, multi-page apps and just about any other modern web technology. ParseHub can handle Javascript, AJAX, cookies, sessions and redirects. You can easily fill in forms, loop through dropdowns, login to websites, click on interactive maps and even deal with infinite scrolling.",
		"website": "https://www.parsehub.com/",
		"twitter": "https://twitter.com/parsehub",
		"github": "https://github.com/parsehub",
		"apisjson_url": "http://theapistack.com/data/parsehub/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/parsehub",
		"base_url": "",
		"blog": "https://blog.parsehub.com/",
		"blogrss": "https://blog.parsehub.com/rss/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/parsehub-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/parsehub-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Deployment"
		]
	},
	{
		"id": "IgVxcWUe0nJAMeslGn1U1nXA07rq123B8AB116eIULWnUXk3tEqR1Iru7wsZN96ftuqbvEhTpVbjg123wrc2FD2VxrlQ117117",
		"name": "PromptCloud",
		"summary": "PromptCloud opeartes on u0026ldquo;Data as a Serviceu0026rdquo; (DaaS) model and deals with large-scale data crawl and extraction, using cutting-edge technologies and cloud computing solutions (Nutch, Hadoop, Lucene, Cassandra, etc). Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format. These data could be from reviews, blogs, produc",
		"details": "PromptCloud opeartes on u0026ldquo;Data as a Serviceu0026rdquo; (DaaS) model and deals with large-scale data crawl and extraction, using cutting-edge technologies and cloud computing solutions (Nutch, Hadoop, Lucene, Cassandra, etc). Its proprietary software employs machine learning techniques to extract meaningful information from the web in desired format. These data could be from reviews, blogs, product catalogs, social sites, travel datau0026mdash;basically anything and everything on WWW. Itu0026rsquo;s a customized solution over simply being a mass-data crawler, so you only get the data you wish to see. The solution provides both deep crawl and refresh crawl of the web pages in a structured format.",
		"website": "http://promptcloud.com/",
		"twitter": "https://twitter.com/promptcloud",
		"github": "https://github.com/promptcloud",
		"apisjson_url": "http://theapistack.com/data/promptcloud/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/promptcloud",
		"base_url": "https://www.promptcloud.com/promptcloud-api-documentation/",
		"blog": "http://blog.promptcloud.com/",
		"blogrss": "http://blog.promptcloud.com/feeds/posts/default?alt=rss",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/prompt-cloud-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/prompt-cloud-logo.png",
		"tags": [
			"Scraping Web Crawler",
			"Scraping",
			"Scraping",
			"API-Stack"
		]
	},
	{
		"id": "V116qWHJ3Qa9V4wwia116kBu4cnPvXeesSkdYJMwxA0RqC6INwhTXGBHWzSt1iP1yMfr5bZ8x4zVBuOYwOwZjcqPfA117117",
		"name": "Saplo",
		"summary": "Saplo uses innovative semantic technologies to analyze text in a way that mimic how humans read and evaluate text. Saplo help organisations extract and refine valuable information hidden in large text collections. Saplo have five different services; Entity Tagging, Topic Tags, Related u0026amp; Similar Articles, Contextual recognition and Sentiment Analysis.",
		"details": "Saplo uses innovative semantic technologies to analyze text in a way that mimic how humans read and evaluate text. Saplo help organisations extract and refine valuable information hidden in large text collections. Saplo have five different services; Entity Tagging, Topic Tags, Related u0026amp; Similar Articles, Contextual recognition and Sentiment Analysis.",
		"website": "http://saplo.com",
		"twitter": "https://twitter.com/saplo",
		"github": "https://github.com/saplo",
		"apisjson_url": "http://theapistack.com/data/saplo/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/saplo",
		"base_url": "http://developer.saplo.com/",
		"blog": "http://saplo.com/blog/",
		"blogrss": "http://feeds.feedburner.com/saplo/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/469_logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/469_logo.png",
		"tags": [
			"Text Analysis",
			"Scraping",
			"Machine Learning Scraping",
			"Content"
		]
	},
	{
		"id": "116JUTarbSrYSaZKcPUozvBrISqrhkGSPepn3iFGLE7nb9u2PXYgUIRlqZOJ6G62GYpzBV8uK5jmIjJepj116Mawmg117117",
		"name": "ScrapeLogo",
		"summary": "ScrapeLogo has been discovered and developed by Maintop Businesses, originally only for internal purposes. It was coded as an independent service for several Maintopu0026rsquo;s B2B projects. When requests from other companies multiplied, a private beta version was launched too. We are now looking for the first beta testers, who would like to show company logos on their websites and help us improve th",
		"details": "ScrapeLogo has been discovered and developed by Maintop Businesses, originally only for internal purposes. It was coded as an independent service for several Maintopu0026rsquo;s B2B projects. When requests from other companies multiplied, a private beta version was launched too. We are now looking for the first beta testers, who would like to show company logos on their websites and help us improve the quality and precision of our algorithm.",
		"website": "http://scrapelogo.com/",
		"twitter": "",
		"github": "",
		"apisjson_url": "http://theapistack.com/data/scrapelogo/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": null,
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/scrape-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/scrape-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Marketing",
			"Logo"
		]
	},
	{
		"id": "Ku1tJw7eZKd3d9123cperVoVCDwpzwx16cgJ1hcE028NjKsf2GPzW6QR123pUGiFhNJL3UqrJHUUUq31VTh5SxuD123g117117",
		"name": "ScraperWiki",
		"summary": "ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. Scraper refers to screen scrapers, programs that extract data from websites. Wiki means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website is pr",
		"details": "ScraperWiki is a web-based platform for collaboratively building programs to extract and analyze public (online) data, in a wiki-like fashion. Scraper refers to screen scrapers, programs that extract data from websites. Wiki means that any user with programming experience can create or edit such programs for extracting new data, or for analyzing existing datasets. The main use of the website is providing a place for programmers and journalists to collaborate on analyzing public data",
		"website": "https://scraperwiki.com/",
		"twitter": "https://twitter.com/scraperwiki",
		"github": "https://github.com/scraperwiki",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/scraperwiki",
		"base_url": "",
		"blog": "https://blog.scraperwiki.com/blog/",
		"blogrss": "https://blog.scraperwiki.com/feed/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/798px-ScraperWiki_logo.png",
		"logo_width": "175",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/798px-ScraperWiki_logo.png",
		"tags": [
			"Spreadsheets",
			"Scraping Tools",
			"Scraping",
			"Harvesting",
			"Hacker Storytelling",
			"Deployment",
			"API-United-Kingdom",
			"API-Scraping-Deployment",
			"Add Tags:   Deployment API Deployment Scraping"
		]
	},
	{
		"id": "R1116k2EkFf78nTAeegnssl9uqHjxhrX53h42MXaeiVWn6s3bvBGsuAYlTTqcRU70UeY5wF7e5hHH0G3Su1230gX2Q117117",
		"name": "Scrapinghub",
		"summary": "Scrapinghub is a company that provides web crawling solutions, including a platform for running crawlers, a tool for building scrapers visually, data feed providers (DaaS) and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures.",
		"details": "Scrapinghub is a company that provides web crawling solutions, including a platform for running crawlers, a tool for building scrapers visually, data feed providers (DaaS) and a consulting team to help startups and enterprises build and maintain their web crawling infrastructures.",
		"website": "http://scrapinghub.com/",
		"twitter": "https://twitter.com/ScrapingHub",
		"github": "https://github.com/scrapinghub",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/scrapinghub",
		"base_url": "",
		"blog": "http://blog.scrapinghub.com/",
		"blogrss": "http://blog.scrapinghub.com/feed/",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/scrapinghub-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/scrapinghub-logo.png",
		"tags": [
			"Scraping-Stacking",
			"Scraping Web Crawler",
			"Scraping Tools",
			"Scraping"
		]
	},
	{
		"id": "pmOJncRLyB5p5NWI36aKwIeTfinPgCpzsDDLNo6obU4nog62VrEn7xGfO5Qq1160lzv9WFtX0bOvl82SXM3FXybw117117",
		"name": "Scrapy",
		"summary": "An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.",
		"details": "An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.",
		"website": "http://scrapy.org/",
		"twitter": "https://twitter.com/ScrapyProject",
		"github": "https://github.com/scrapy",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/scrapy",
		"base_url": "",
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Scrapy_logo.jpg",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/Scrapy_logo.jpg",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Harvesting"
		]
	},
	{
		"id": "MDLBQ1nKzTqnEsInH508dPPRNkOu7npfqggCUCQpK64iHsUzwEyAriHkzVzilctZcr6Hr7KUcvvnLpRHrZB123uQ117117",
		"name": "Screen Scraper",
		"summary": "Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).",
		"details": "Copying text from a web page. Clicking links. Entering data into forms and submitting. Iterating through search results pages. Downloading files (PDF, MS Word, images, etc.).",
		"website": "http://screen-scraper.com/",
		"twitter": "",
		"github": "",
		"apisjson_url": "http://theapistack.com/data/screen-scraper/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": null,
		"blog": "http://blog.screen-scraper.com/",
		"blogrss": "http://feeds.feedburner.com/screen-scrapeable",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/screen-scraper-logo.png",
		"logo_width": "175",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/screen-scraper-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Scraping",
			"API-Stack"
		]
	},
	{
		"id": "uI6pkmQZZTtoB2TfcSTyL673f1dc86p7OmU7TgZfRYgu3RnecKKOAKm7eelSnzwAkdE7N9iobgSlCm86yKS43g117117",
		"name": "TextRazor",
		"summary": "The service provides analysis of selected text passages to identify named entities and statements of fact with disambiguation to distinguish similar text strings. It applies machine learning algorithms and natural language processing to connect a text sample with a knowledge base and identify known elements and their relationships. API methods support submission of a text sample to be parsed.u0026nbsp",
		"details": "The service provides analysis of selected text passages to identify named entities and statements of fact with disambiguation to distinguish similar text strings. It applies machine learning algorithms and natural language processing to connect a text sample with a knowledge base and identify known elements and their relationships. API methods support submission of a text sample to be parsed.u0026nbsp;",
		"website": "http://www.textrazor.com",
		"twitter": "https://twitter.com/TextRazor",
		"github": "https://github.com/TextRazor",
		"apisjson_url": "http://theapistack.com/data/textrazor/apis.json",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "https://github.com/TextRazor",
		"base_url": "http://www.textrazor.com/documentation_rest",
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/textrazor.gif",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/textrazor.gif",
		"tags": [
			"Target",
			"Semantic",
			"Scraping",
			"Machine Learning Scraping",
			"Machine Learning",
			"Content"
		]
	},
	{
		"id": "m9KSY8Y123slwwjIuypI22wPDP0QjUfLo1y64v116dJhZf6b3FvCcfJCzuGekVXNICVVeGwurpKds5gVf1166r3BXuJw117117",
		"name": "WrapAPI",
		"summary": "Build an API on top of any website. Turn any website...into a parameterized APIBuild, share, and use APIs made from webpages. Use WrapAPI to scrape sites, build better UIs, and automate online tasks.",
		"details": "Build an API on top of any website. Turn any website...into a parameterized APIBuild, share, and use APIs made from webpages. Use WrapAPI to scrape sites, build better UIs, and automate online tasks.",
		"website": "https://wrapapi.com",
		"twitter": "",
		"github": "",
		"apisjson_url": "",
		"sdksio_url": "",
		"postman_url": "",
		"portal_url": "",
		"base_url": "",
		"blog": "",
		"blogrss": "",
		"logo": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/wrapapi-logo.png",
		"logo_width": "150",
		"screenshot": "http://kinlane-productions.s3.amazonaws.com/api-evangelist-site/company/logos/wrapapi-logo.png",
		"tags": [
			"Scraping Tools",
			"Scraping",
			"Deployment"
		]
	}
]